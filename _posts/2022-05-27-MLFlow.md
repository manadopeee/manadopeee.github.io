---
layout: post
title: "Machine Learning 흐름"
author: kei
categories: [ 머신러닝, 기초 ]
<!-- image: assets/images/17.jpg -->
<!-- beforetoc: "머신러닝의 학습에 있어서 필요한 함수의 역할과 순전파, 역전파의 개념 " -->
toc: true
---
## Data Preprocessing
> 데이터전처리

## Activation Function
> input과 weight(초기값은 random)의 곱이 활성화 함수를 통해 어떤 임계값이 넘는 값이 들어오면\
> 의미있는 값으로 판단하고 다음 레이어에 값을 제공\
> \
> input과 output의 값을 모두 알고 있다면 weight를 찾아내는 것이 가능\
> 비선형 함수

## Loss Function
> 순전파(forward propagation) 통해 나온 예측값(prediction)을 실제값(label)과 손실 함수를 통해 비교

## Back Propagation
> Loss값이 최소로 되는 방향(오차가 감소)으로 weight를 계산\
> 전체 오차(Loss Function의 output)의 미분값에 learning rate를 곱한 후 기존의 가중치(순전파에서 나온 weight)에서 빼준다\
> activationFunction(input x w)을 출력값(Loss Function의 output)으로 미분\
> output layer에서 input layer 방향으로 실행

## Optimizer
> Loss 값이 최소(기울기가 0)로 나오기 위해 역전파로 계산을 하는데 그러기 위해선 역전파에서 weight를 미분해야한다\
> 가중치(w)의 그래디언트(미분값)가 최소로 되는 지점이 손실 함수의 최소값(지점)이라고 가정\
> 여기서 어떻게 미분을 할 것인가를 정하는 것이 옵티마이저

## Reference
<a href="https://blog.naver.com/PostView.nhn?blogId=riverrun17&logNo=221901319498">#4 Relu, Sigmoid 함수 역전파(미분)</a>\
<a href="https://velog.io/@thwjd639/Backpropagation-%EC%97%AD%EC%A0%84%ED%8C%8C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98">Backpropagation 역전파 알고리즘</a>\
<a href="https://goofcode.github.io/back-propagation">딥러닝 기초 - 오차역전파(back propagation) 알고리즘</a>\
<a href="https://www.datamaker.io/posts/32/">오차 역전파(Back-propagation)</a>\
<a href="https://box-world.tistory.com/19">[머신러닝] Back Propagation(역전파) 정복하기</a>\
<a href="https://value-error.tistory.com/54">밑바닥부터 시작하는 딥러닝(오차역전파법, affine층 역전파, simoid, Relu)</a>\
<a href="https://dosundosun.tistory.com/14">딥러닝 :: 오차 역전파, 활성화 함수, 고급 경사 하강법</a>\
<a href="https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=handuelly&logNo=221824080339">딥러닝 - 활성화 함수(Activation) 종류 및 비교</a>

<!-- <figure>
   <a href="https://velog.io/@thwjd639/Backpropagation-%EC%97%AD%EC%A0%84%ED%8C%8C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98">
   <img src="logo.png" style="max-width: 200px;" alt="Jekyll logo" />
   </a>
   <figcaption>Backpropagation 역전파 알고리즘</figcaption>
</figure> -->
